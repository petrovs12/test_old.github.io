{"Engineering/Interviewing/Ninja/List_3_Int_Add_to_Zero":{"title":"List_3_Int_Add_to_Zero","links":[],"tags":[],"content":""},"Engineering/Interviewing/WritingGoodFeedback":{"title":"WritingGoodFeedback","links":[],"tags":[],"content":","},"about":{"title":"About This Site","links":[],"tags":[],"content":"About This Digital Garden\nWelcome to my digital garden! This is where I share my notes, thoughts, and learnings about various topics including:\n\nMathematics and Statistics\nMachine Learning and Data Science\nSystem Design and Engineering\nAnd more…\n\nWhat is a Digital Garden?\nA digital garden is a collection of notes, essays, and ideas that grow over time. Unlike traditional blogs, digital gardens:\n\nAre living documents that evolve\nDon’t have to be perfect or complete\nShow connections between different ideas\nGrow organically over time\n\nHow This Site Works\nThis site is built using:\n\nObsidian for note-taking\nQuartz for publishing\nGitHub Pages for hosting\n\nFeel free to explore the interconnected notes and thoughts!"},"index":{"title":"Welcome to My Knowledge Base","links":["about","test","science.math","science.stats"],"tags":[],"content":"Welcome to My Knowledge Base\nThis is my personal knowledge base where I share my notes and thoughts on various topics.\nRecent Notes\n\nAbout This Site\nTest Note\nMathematics\nStatistics\n\nMain Categories\n\nMathematics and Statistics\nMachine Learning\nEngineering\nSystem Design\n\nAbout\nThis is a digital garden built with Obsidian and Quartz. Feel free to explore the interconnected notes and thoughts. Learn more in the About page."},"science.stats.AB-Testing":{"title":"A/B Testing or Experimental Design","links":["seed.Product-Management.Product-Metrics.User-Based-Metrics-(Digital-Products)","science.stats.tests","science.math.Game-Theory.Agent-Based-Models","science.stats.Regression.Ranking","engineering.system_design.ML-System-Design.Recommender-Systems","science.stats.Regression.Loss-Functions"],"tags":[],"content":"From DataInterviewPro\nControlled experiments (e.g. clinical trials).\nUsually when we run experiments, we want to see difference in certain seed.Product Management.Product Metrics.User Based Metrics (Digital Products), e.g. Transclude of seed.Product-Management.Product-Metrics.User-Based-Metrics-(Digital-Products)#^active-users.\nTransclude of science.stats.tests#^ttest can be used in the simplest case, or some bayesian setup.\nType II error (or power). Power = 1 - Type II error.\nsample_size \\approx \\frac{16\\sigma^2}{\\tau^2} where \\tau is the effect size (difference in the metric chosen between the control and treatment group).\nAs we don’t know \\tau, we can choose the minimal detectable effect size, equal to the minimal business-relevant effect size, i.e. the min effect that would justify a switch.\nNormally decided by stakeholders.\nMultiple Testing Problems\nBayesian setup, Bonferroni correction, or something else.\nBonferroni correction:\ndivide significance level by the number of tests.\nBut sometimes too conservative, i.e. lower recall\nOr can control False Discovery Rate.\nFDR = E[\\frac{falsePositives}{rejectiosn}].\nThis can be calculated, then choose a threshold for FDR, etc.\nIn a decision-making setup, can optimize the above…\nNovelty and Primacy Effect\nUser behavior can either be resistent to change, or welcoming it, and we don’t know apriori.\nIf the effect is initially large up/down and then gets closer to the baseline, this is the novelty/primacy effect.\nRun Longer\nUse Only New Users (or equivalent type matching)\nCan fix by running experiments on new users, or having the baseline be the behavior or new users when they were new, etc. In the latter case should maybe control for seasonality or other temporal effects as appropriate.\nInterference between variants\nHappens most often when control and treatment groups interact somehow.\nIf they interact like a ‘standard’ network effect (not competing for resources), we expect the actual effect to be larger than during the test.\nExample FB posts promote more posts, so if small increase in population, will get larger.\nscience.math.Game Theory.Agent Based Models\nTransclude of science.math.Game-Theory.Agent-Based-Models#^fbepidemology\nIf 2-sided market, control and treatment groups compete for same resource. If we give better ‘weapon’ for treatment, after launch it would disappear, as everyone has better “weapon”.\nExample Better matching algo for Uber drivers. (or in the extreme case, prioritize some drivers over others- obviously the treatment group is more priviliged).\nIsolation of effect\nTest between “connected components”, e.g. New York vs LA (approximately, of course). Also can do temporally etc.\nSimplest is geographical splits.\nIn social network:\ntest on differennt ‘clusters’ of people.\nEgo-network randomization. A more scalable variation of the above.\nAB testing for science.stats.Regression.Ranking\nengineering.system_design.ML System Design.Recommender Systems\nPlease also see ^ranking-start.\nComparing two rankings via clicks.\nSay 2 algorithms give 2 rankings, Ranking A, ranking B.\nMerge-sort-remove-duplicates interleaving the rankings.\nAssign first element randomly.\nCount clicks from A and B. Better one will on average get more rankings."},"science.stats.Confidence-Intervals":{"title":"Confidence Intervals","links":[],"tags":[],"content":"In a frequentist setting, confidence intervals can be taken from the MLE + CLT via the identity:\nP(-z_{\\alpha/2}&lt;\\sqrt(I(\\theta)*(\\hat{\\theta}-\\theta)) &lt; z_{\\alpha/2})\\approx 1-\\alpha\nWhere z_{alpha} is the 1-\\alpha- th quantile of the standard normal distribution:\nz_{\\alpha} = \\Phi^{-1}(1-\\alpha),\nthat is:\nP_{x\\approx N(0,1)}(x&gt;z_{alpha})=\\alpha"},"science.stats.Correlation-and-Covariance":{"title":"Large Correlation Matrices in R","links":[],"tags":[],"content":"var(v) = \\Sigma(v_i-E(v))^2 = E[(X-\\mu)^2]\ncov(X,Y) = \\Sigma((x_i-E(x))(y_i-E(y)))/n\nor (n-1) for sample covariance\ncor(X,Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)*var(Y)}}\nPearson correlation\nblog post about bigCor"},"science.stats.Deep-Neural-Networks.Attention-Mechanisms":{"title":"Bahnadau Attention mechanism","links":[],"tags":[],"content":"Let’s start with training inputs x_1,x_2,...,x_n as input sequence and y_1,y_2,...,y_n as target sequence.\nThen we will have the following model:\n\nCompute h_i = concat(h_{i,forward},h_{i,backward}), encoding of 2 LSTM’s, one ran forward and one ran backward only on x- the input sequence.\nInitialize s_0 randomly\nDefine a_{0,j} = softmax(s_0,h_j  for j=1..N.\nDefine now c_1 = \\Sigma_{j=1..N}h_j*a_{0,j}\nPostulate s_1 = f(s_0,y_0,c_1)\nPostulate y_1 = g(y_0,s_1,c_1)\n\nUnroll the above in a loop, so it becomes:\n\nDefine a_{t,j} = softmax(s_{t-1},h_j  for j=1..N.\nDefine now c_t = \\sum_{j=1..N}h_j*a_{t,j}\nPostulate s_t = f(s_{t-1},y_{t-1},c_t)\nPostulate y_t = g(y_{t-1},s_t,c_t)\n\nNote now after training we only use y in the last 2 equations, and y_t appears only on the left-hand side of the last equation.\nThis gives a way to generate y.\nSimulate ‘time flow’ with masking.\nTransformer Tutorial from\nwww.jku.at/fileadmin/gruppen/173/Research/Introduction_to_Transformers.pdf\nRNN Recap\nOutput h^{(t)} is a function of input e^{(t)} and the\noutput of the previous time step h^{(t-1)}.\nh(t) = RNN(h(t-1), e(t))\nh^t - is hidden state\nIf part of a whole, we can say h^t is a contextualized embedding of e^t.\nAttention Networks\nGeneral Form :\nO=Att(Q,V)\nwhere all of the above are whole matrices:\n\n\nGiven a set of vector values V, and a set of vector\nqueries Q, attention is a technique to compute a\nweighted sum of the values, dependent on each query.\nThe weights in the weighted sums are called attentions\nand denoted by \\alpha.\nSo, then:\n\\alpha_{i,j} - weight for “query” i on “value” j.\nalpha_i - attention for “query” i.\n\\sum(\\alpha_i)=1.\n\\alpha_{i,j} = f(q_i,v_j)\nwhere f is some neural network.\nThe output o_i= \\sum_{j=1..|V|}alpha_{i,j}*v_j\nAttention Variants\nUnnormalized dot product attention.\nLet\n\\bar{a}_{i,j} = q_i*v_j\na_{i,j} = \\frac{exp(a_{i,j})}{\\sum_j(exp(a_{i,j}))}\nv^{out}_j = \\sum_{j=1..}a_{i,j}*v_j\nis then the contextual representation\nThe scaled version would be:\n\\bar{a}_{i,j} = \\frac{q_i*v_j}{\\sqrt(d)}\nAnother version w/ some paramters would be:\n\\bar{a}_{i,j} = q_i*\\bold{W}*v_j where W is a parameter matrix.\nProblem w/ single- head attention is that softmax is quite ‘sharp’ and maybe there are multiple ‘concepts’ to learn (e.g. one is subject-object relation, another subject-verb, etc).\nIn multi-head attention, we can pre-project the embedding into a smaller space, and then concat the outputs."},"science.stats.Deep-Neural-Networks.Embeddings":{"title":"Embeddings","links":["science.stats.Latent-Variables","science.stats.Dimensionality-Reduction","science.stats.Deep-Neural-Networks.Search.RAG","science.stats.Dimensionality-Reduction.t-SNE-(T-distributed-stochastic-neighbor-embedding)"],"tags":[],"content":"Embedding is the same thing as discovering a science.stats.Latent Variables representation and the goal is meaningful\nscience.stats.Dimensionality Reduction.\nWord Embeddings in neural nets\nnewral nets adaptation ot other classes\nSearch\nscience.stats.Deep Neural Networks.Search.RAG\nscience.stats.Dimensionality Reduction.t-SNE (T-distributed stochastic neighbor embedding) and other stuff…"},"science.stats.Deep-Neural-Networks.Language-Models":{"title":"Language Modelling","links":["science.stats.Deep-Neural-Networks.Transformers"],"tags":[],"content":"Predict the next word or charater in a document. Can be used downstream for almost anything:\n\nText Classification\nQuestion Answering\nSentiment Analysis.\nText Generation.\n\nTop Models:GPT-3, GPT-2, Megatron-ML.\nscience.stats.Deep Neural Networks.Transformers wildly used.\nCan use Bert out of the box quickly and easily.\ngithub.com/onnx/models#machine_comprehension\nWord2Vec ^word2vec\nDoc2Vec ^doc2vec\nMachine Comprehension\nGPT-2/3\nBERT\nMachine Translation\nTransformer Cycle model\nConversational Interfaces"},"science.stats.Deep-Neural-Networks":{"title":"Hierarchical Modelling","links":["science.CS.theory.Code-Transformations.Differentiable-Programming.tensorflow","science.CS.theory.Code-Transformations.Differentiable-Programming.pytorch","science.math.theory.Dynamical-Systems","science.math.mode","science.stats.Autoregressive-Models","Philosophy-and-Rationality.Incentives-for-Research-and-Engineering-Advances^","tags/TODO","science.stats.Regression.Recommender-Systems","^feed-forward-nets","^symetries","science.stats.Deep-Neural-Networks.Language-Models","science.stats.Deep-Neural-Networks.Prod2Vec","science.stats.Deep-Neural-Networks","science.stats.Deep-Neural-Networks.Max-Pooling","science.stats.Deep-Neural-Networks.Auto-encoders","science.math.Optimization.Optimizers-In-in-Neural-Networks.ReLU","science.stats.Stacking-Models","science.stats.Regression.Support-Vector-Machines","science.stats.Regression.Support-Vector-Machines.Factorization-Machines","science.CS.theory.Code-Transformations.Differentiable-and-Probabalistic-Programming","science.CS.theory.Code-Transformations.Differentiable-Programming.jax","science.cs.languages.CUDA"],"tags":["TODO"],"content":"science.CS.theory.Code Transformations.Differentiable Programming.tensorflow\nscience.CS.theory.Code Transformations.Differentiable Programming.pytorch\nStatistical View on Deep Neural Networks\nRecurrent Neural Networks\nThe exploding/vanishing gradient problem\nscience.math.theory.Dynamical Systems\nscience.math.mode\nscience.stats.Autoregressive Models\nSymetries ^symmetries\nRestricted Boltzmann Machines\nSome of the best single-model performers in netflix-prize.\nTODO add pages for the architectures and notes here\nFeed Forward Neural Networks ^feed-forward-nets\nDeep-and-wide networks\nDeep Hierarchical model for numeric features, sparse and ‘skipping layers’ for categorical features and their products…\n\nLately being used in science.stats.Regression.Recommender Systems. There are claims they perform well…\nIn R formula language parlance it’s a bit like this:\nx~Bernoulli_logit(f(numeric_features)+categorical_features^2)\nwhere f is deep ^feed-forward-nets\nor so. The categorical features^2 part could be specied further, of course, and the\nBernoulli_logit sigmoid layer could be skipped.\nYou have to specify what feature interactions to directly include.\nConvolutional Neural Networks ^cnn\n^symetries\nAlexNet ^alexnet\nRecurrent Neural Networks\n\nRNNs\nMachine Translation, text generation\nLSTM,GRU\n\nEmbeddings\n^word2vec\nTransclude of science.stats.Deep-Neural-Networks.Prod2Vec#^prod2vec\nDeep Learning as Feature Extraction\nRecommendation Systems (DL)\nDeep Content-based Models\nSpotify Example\n\n^cnn with audio spectrogram as input data.\nscience.stats.Deep Neural Networks.Max Pooling and global pooling\n‘Bottleneck’ layer science.stats.Deep Neural Networks.Auto-encoders\nscience.math.Optimization.Optimizers In in Neural Networks.ReLU\nOutput is the factor vector for the track from a trained Collaborative Filtering model. It’s a model stack…science.stats.Stacking Models\n\nDeep Factorization Machines\nscience.stats.Regression.Support Vector Machines\nscience.stats.Regression.Support Vector Machines.Factorization Machines\n#TODO - skip for now\nContent2Vec ^content2vec\nCombine modular sets of feature extractors into 1 item embedding. E.g.\n^alexnet,^word2vec, science.stats.Deep Neural Networks.Prod2Vec, etc.\nFrameworks/Libraries\nscience.CS.theory.Code Transformations.Differentiable and Probabalistic Programming\nscience.CS.theory.Code Transformations.Differentiable Programming.tensorflow\nscience.CS.theory.Code Transformations.Differentiable Programming.pytorch\nscience.CS.theory.Code Transformations.Differentiable Programming.jax\nscience.cs.languages.CUDA"},"science.stats.Estimators":{"title":"Estimators","links":["science.stats.Regression.Linear-Regression"],"tags":["type/sketchnote","type/note","theme/xyz"],"content":"\n\nAn estimator is a statistic (function of data) that outputs a value for some parameter of the data generating process.\nExamples:\nAnalytical Solution\n\ndv.paragraph(dv.current().visual);\n\n\n\n                  \n                  Note\n                  \n                \n\n= this.lead\n\n\nDetails\n\n\n\n\nSupporting Content\n\n\n\n\n\nBack Matter\nSource\n \n\nbased_on::\n\nReferences\n\n\n\n\nTerms\n\n\n\n\nTarget\n\n\n\n\nTasks\n \n\n\n\nQuestions\n \n\n\n\nTemplate Help\n\n\nBasic Template Structure\nHow to Use Links\nHow to Use Tags\nHow to Search Notes\nPlugins Needed\nFind Latest Updates\n"},"science.stats":{"title":"Stats","links":[],"tags":[],"content":""}}