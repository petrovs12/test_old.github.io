{"Engineering/Interviewing/Ninja/List_3_Int_Add_to_Zero":{"title":"List_3_Int_Add_to_Zero","links":[],"tags":[],"content":""},"Engineering/Interviewing/WritingGoodFeedback":{"title":"WritingGoodFeedback","links":[],"tags":[],"content":","},"about":{"title":"About This Site","links":[],"tags":[],"content":"About This Digital Garden\nWelcome to my digital garden! This is where I share my notes, thoughts, and learnings about various topics including:\n\nMathematics and Statistics\nMachine Learning and Data Science\nSystem Design and Engineering\nAnd more…\n\nWhat is a Digital Garden?\nA digital garden is a collection of notes, essays, and ideas that grow over time. Unlike traditional blogs, digital gardens:\n\nAre living documents that evolve\nDon’t have to be perfect or complete\nShow connections between different ideas\nGrow organically over time\n\nHow This Site Works\nThis site is built using:\n\nObsidian for note-taking\nQuartz for publishing\nGitHub Pages for hosting\n\nFeel free to explore the interconnected notes and thoughts!"},"index":{"title":"Welcome to My Knowledge Base","links":["about","test","science.math","science.stats"],"tags":[],"content":"Welcome to My Knowledge Base\nThis is my personal knowledge base where I share my notes and thoughts on various topics.\nRecent Notes\n\nAbout This Site\nTest Note\nMathematics\nStatistics\n\nMain Categories\n\nMathematics and Statistics\nMachine Learning\nEngineering\nSystem Design\n\nAbout\nThis is a digital garden built with Obsidian and Quartz. Feel free to explore the interconnected notes and thoughts. Learn more in the About page."},"science.stats.AB-Testing":{"title":"A/B Testing or Experimental Design","links":["seed.Product-Management.Product-Metrics.User-Based-Metrics-(Digital-Products)","science.stats.tests","science.math.Game-Theory.Agent-Based-Models","science.stats.Regression.Ranking","engineering.system_design.ML-System-Design.Recommender-Systems","science.stats.Regression.Loss-Functions"],"tags":[],"content":"From DataInterviewPro\nControlled experiments (e.g. clinical trials).\nUsually when we run experiments, we want to see difference in certain seed.Product Management.Product Metrics.User Based Metrics (Digital Products), e.g. Transclude of seed.Product-Management.Product-Metrics.User-Based-Metrics-(Digital-Products)#^active-users.\nTransclude of science.stats.tests#^ttest can be used in the simplest case, or some bayesian setup.\nType II error (or power). Power = 1 - Type II error.\nsample_size \\approx \\frac{16\\sigma^2}{\\tau^2} where \\tau is the effect size (difference in the metric chosen between the control and treatment group).\nAs we don’t know \\tau, we can choose the minimal detectable effect size, equal to the minimal business-relevant effect size, i.e. the min effect that would justify a switch.\nNormally decided by stakeholders.\nMultiple Testing Problems\nBayesian setup, Bonferroni correction, or something else.\nBonferroni correction:\ndivide significance level by the number of tests.\nBut sometimes too conservative, i.e. lower recall\nOr can control False Discovery Rate.\nFDR = E[\\frac{falsePositives}{rejectiosn}].\nThis can be calculated, then choose a threshold for FDR, etc.\nIn a decision-making setup, can optimize the above…\nNovelty and Primacy Effect\nUser behavior can either be resistent to change, or welcoming it, and we don’t know apriori.\nIf the effect is initially large up/down and then gets closer to the baseline, this is the novelty/primacy effect.\nRun Longer\nUse Only New Users (or equivalent type matching)\nCan fix by running experiments on new users, or having the baseline be the behavior or new users when they were new, etc. In the latter case should maybe control for seasonality or other temporal effects as appropriate.\nInterference between variants\nHappens most often when control and treatment groups interact somehow.\nIf they interact like a ‘standard’ network effect (not competing for resources), we expect the actual effect to be larger than during the test.\nExample FB posts promote more posts, so if small increase in population, will get larger.\nscience.math.Game Theory.Agent Based Models\nTransclude of science.math.Game-Theory.Agent-Based-Models#^fbepidemology\nIf 2-sided market, control and treatment groups compete for same resource. If we give better ‘weapon’ for treatment, after launch it would disappear, as everyone has better “weapon”.\nExample Better matching algo for Uber drivers. (or in the extreme case, prioritize some drivers over others- obviously the treatment group is more priviliged).\nIsolation of effect\nTest between “connected components”, e.g. New York vs LA (approximately, of course). Also can do temporally etc.\nSimplest is geographical splits.\nIn social network:\ntest on differennt ‘clusters’ of people.\nEgo-network randomization. A more scalable variation of the above.\nAB testing for science.stats.Regression.Ranking\nengineering.system_design.ML System Design.Recommender Systems\nPlease also see ^ranking-start.\nComparing two rankings via clicks.\nSay 2 algorithms give 2 rankings, Ranking A, ranking B.\nMerge-sort-remove-duplicates interleaving the rankings.\nAssign first element randomly.\nCount clicks from A and B. Better one will on average get more rankings."},"science.stats.Confidence-Intervals":{"title":"Confidence Intervals","links":[],"tags":[],"content":"In a frequentist setting, confidence intervals can be taken from the MLE + CLT via the identity:\nP(-z_{\\alpha/2}&lt;\\sqrt(I(\\theta)*(\\hat{\\theta}-\\theta)) &lt; z_{\\alpha/2})\\approx 1-\\alpha\nWhere z_{alpha} is the 1-\\alpha- th quantile of the standard normal distribution:\nz_{\\alpha} = \\Phi^{-1}(1-\\alpha),\nthat is:\nP_{x\\approx N(0,1)}(x&gt;z_{alpha})=\\alpha"},"science.stats.Correlation-and-Covariance":{"title":"Large Correlation Matrices in R","links":[],"tags":[],"content":"var(v) = \\Sigma(v_i-E(v))^2 = E[(X-\\mu)^2]\ncov(X,Y) = \\Sigma((x_i-E(x))(y_i-E(y)))/n\nor (n-1) for sample covariance\ncor(X,Y) = \\frac{cov(X,Y)}{\\sqrt{var(X)*var(Y)}}\nPearson correlation\nblog post about bigCor"},"science.stats.Deep-Neural-Networks.Attention-Mechanisms":{"title":"Bahnadau Attention mechanism","links":[],"tags":[],"content":"Let’s start with training inputs x_1,x_2,...,x_n as input sequence and y_1,y_2,...,y_n as target sequence.\nThen we will have the following model:\n\nCompute h_i = concat(h_{i,forward},h_{i,backward}), encoding of 2 LSTM’s, one ran forward and one ran backward only on x- the input sequence.\nInitialize s_0 randomly\nDefine a_{0,j} = softmax(s_0,h_j  for j=1..N.\nDefine now c_1 = \\Sigma_{j=1..N}h_j*a_{0,j}\nPostulate s_1 = f(s_0,y_0,c_1)\nPostulate y_1 = g(y_0,s_1,c_1)\n\nUnroll the above in a loop, so it becomes:\n\nDefine a_{t,j} = softmax(s_{t-1},h_j  for j=1..N.\nDefine now c_t = \\sum_{j=1..N}h_j*a_{t,j}\nPostulate s_t = f(s_{t-1},y_{t-1},c_t)\nPostulate y_t = g(y_{t-1},s_t,c_t)\n\nNote now after training we only use y in the last 2 equations, and y_t appears only on the left-hand side of the last equation.\nThis gives a way to generate y.\nSimulate ‘time flow’ with masking.\nTransformer Tutorial from\nwww.jku.at/fileadmin/gruppen/173/Research/Introduction_to_Transformers.pdf\nRNN Recap\nOutput h^{(t)} is a function of input e^{(t)} and the\noutput of the previous time step h^{(t-1)}.\nh(t) = RNN(h(t-1), e(t))\nh^t - is hidden state\nIf part of a whole, we can say h^t is a contextualized embedding of e^t.\nAttention Networks\nGeneral Form :\nO=Att(Q,V)\nwhere all of the above are whole matrices:\n\n\nGiven a set of vector values V, and a set of vector\nqueries Q, attention is a technique to compute a\nweighted sum of the values, dependent on each query.\nThe weights in the weighted sums are called attentions\nand denoted by \\alpha.\nSo, then:\n\\alpha_{i,j} - weight for “query” i on “value” j.\nalpha_i - attention for “query” i.\n\\sum(\\alpha_i)=1.\n\\alpha_{i,j} = f(q_i,v_j)\nwhere f is some neural network.\nThe output o_i= \\sum_{j=1..|V|}alpha_{i,j}*v_j\nAttention Variants\nUnnormalized dot product attention.\nLet\n\\bar{a}_{i,j} = q_i*v_j\na_{i,j} = \\frac{exp(a_{i,j})}{\\sum_j(exp(a_{i,j}))}\nv^{out}_j = \\sum_{j=1..}a_{i,j}*v_j\nis then the contextual representation\nThe scaled version would be:\n\\bar{a}_{i,j} = \\frac{q_i*v_j}{\\sqrt(d)}\nAnother version w/ some paramters would be:\n\\bar{a}_{i,j} = q_i*\\bold{W}*v_j where W is a parameter matrix.\nProblem w/ single- head attention is that softmax is quite ‘sharp’ and maybe there are multiple ‘concepts’ to learn (e.g. one is subject-object relation, another subject-verb, etc).\nIn multi-head attention, we can pre-project the embedding into a smaller space, and then concat the outputs."},"science.stats.Deep-Neural-Networks.Embeddings":{"title":"Embeddings","links":["science.stats.Latent-Variables","science.stats.Dimensionality-Reduction","science.stats.Deep-Neural-Networks.Search.RAG","science.stats.Dimensionality-Reduction.t-SNE-(T-distributed-stochastic-neighbor-embedding)"],"tags":[],"content":"Embedding is the same thing as discovering a science.stats.Latent Variables representation and the goal is meaningful\nscience.stats.Dimensionality Reduction.\nWord Embeddings in neural nets\nnewral nets adaptation ot other classes\nSearch\nscience.stats.Deep Neural Networks.Search.RAG\nscience.stats.Dimensionality Reduction.t-SNE (T-distributed stochastic neighbor embedding) and other stuff…"},"science.stats.Deep-Neural-Networks.Language-Models":{"title":"Language Modelling","links":["science.stats.Deep-Neural-Networks.Transformers"],"tags":[],"content":"Predict the next word or charater in a document. Can be used downstream for almost anything:\n\nText Classification\nQuestion Answering\nSentiment Analysis.\nText Generation.\n\nTop Models:GPT-3, GPT-2, Megatron-ML.\nscience.stats.Deep Neural Networks.Transformers wildly used.\nCan use Bert out of the box quickly and easily.\ngithub.com/onnx/models#machine_comprehension\nWord2Vec ^word2vec\nDoc2Vec ^doc2vec\nMachine Comprehension\nGPT-2/3\nBERT\nMachine Translation\nTransformer Cycle model\nConversational Interfaces"},"science.stats.Deep-Neural-Networks.Vision-Models.Image-Diffusion-Models":{"title":"Image Diffusion Model","links":[],"tags":[],"content":"denoising models\nonce trained, the gdiffusion model is able to generate images from gaussian noise…\ndiffusion models\nvery successful, denoising process, trian by\nadding noise to images.\nadd some amount of noise to the image\ndepending on how much you add, you can be close or not to the image…\nflowchart TD\n    A[Input Data] --&gt; B[Encoder]\n    B --&gt; C{Latent Space Z}\n    C --&gt; D[Reparameterization]\n    D --&gt; E[Decoder]\n    E --&gt; F[Reconstructed Data]\n    A -- Reconstruction Loss --&gt; G[ELBO Calculation]\n    C -- KL Divergence --&gt; G\n    F -- Output --&gt; H[Final Loss]\n"},"science.stats.Deep-Neural-Networks.Vision-Models":{"title":"Image Classification","links":["science.stats.Deep-Neural-Networks","science.engineering.technologies.MLOps.ONNX"],"tags":[],"content":"^cnn\nNormally these models are trained on ImageNet and top-5 classification accuracy is reported.\nHere are some of the current champions that are also available as science.engineering.technologies.MLOps.ONNX models.\nVGG-16\n\nKeras Link\nResNet\nUses Shortcut connections\nEfficientNet-Lite4\nPaper Link\nONXX\nObject detection and Image Segmentation\nObject detection and image segmentation algorithms draw rectangles on a picture and put labels on them.\nEvaluation metrics\nA common evaluation metric are (from here):\nIntersection over Union (IoU):\nEvaluate the overlap of the two bounding boxes. Requires a true and predicted bounding box. So then we have something like:\nIntersection Over Union on top 5 Labels.\nCOCO mAP\nFor the COCO 2017 challenge, the mAP was calculated by averaging the AP over all 80 object categories AND all 10 IoU thresholds from 0.5 to 0.95 with a step size of 0.05. The authors hypothesize that averaging over IoUs rewards detectors with better localization.\nModels/Algorithms\nYolo3\nYoloV4\nBody, Face, And Gesture Analysis\nImage MNanipulation\n\nUnpaired Image To Image Translation\nSuper REsolution\nFast Neural Style Transfer\n"},"science.stats.Deep-Neural-Networks":{"title":"Hierarchical Modelling","links":["science.CS.theory.Code-Transformations.Differentiable-Programming.tensorflow","science.CS.theory.Code-Transformations.Differentiable-Programming.pytorch","science.math.theory.Dynamical-Systems","science.math.mode","science.stats.Autoregressive-Models","Philosophy-and-Rationality.Incentives-for-Research-and-Engineering-Advances^","tags/TODO","science.stats.Regression.Recommender-Systems","^feed-forward-nets","^symetries","science.stats.Deep-Neural-Networks.Language-Models","science.stats.Deep-Neural-Networks.Prod2Vec","science.stats.Deep-Neural-Networks","science.stats.Deep-Neural-Networks.Max-Pooling","science.stats.Deep-Neural-Networks.Auto-encoders","science.math.Optimization.Optimizers-In-in-Neural-Networks.ReLU","science.stats.Stacking-Models","science.stats.Regression.Support-Vector-Machines","science.stats.Regression.Support-Vector-Machines.Factorization-Machines","science.CS.theory.Code-Transformations.Differentiable-and-Probabalistic-Programming","science.CS.theory.Code-Transformations.Differentiable-Programming.jax","science.cs.languages.CUDA"],"tags":["TODO"],"content":"science.CS.theory.Code Transformations.Differentiable Programming.tensorflow\nscience.CS.theory.Code Transformations.Differentiable Programming.pytorch\nStatistical View on Deep Neural Networks\nRecurrent Neural Networks\nThe exploding/vanishing gradient problem\nscience.math.theory.Dynamical Systems\nscience.math.mode\nscience.stats.Autoregressive Models\nSymetries ^symmetries\nRestricted Boltzmann Machines\nSome of the best single-model performers in netflix-prize.\nTODO add pages for the architectures and notes here\nFeed Forward Neural Networks ^feed-forward-nets\nDeep-and-wide networks\nDeep Hierarchical model for numeric features, sparse and ‘skipping layers’ for categorical features and their products…\n\nLately being used in science.stats.Regression.Recommender Systems. There are claims they perform well…\nIn R formula language parlance it’s a bit like this:\nx~Bernoulli_logit(f(numeric_features)+categorical_features^2)\nwhere f is deep ^feed-forward-nets\nor so. The categorical features^2 part could be specied further, of course, and the\nBernoulli_logit sigmoid layer could be skipped.\nYou have to specify what feature interactions to directly include.\nConvolutional Neural Networks ^cnn\n^symetries\nAlexNet ^alexnet\nRecurrent Neural Networks\n\nRNNs\nMachine Translation, text generation\nLSTM,GRU\n\nEmbeddings\n^word2vec\nTransclude of science.stats.Deep-Neural-Networks.Prod2Vec#^prod2vec\nDeep Learning as Feature Extraction\nRecommendation Systems (DL)\nDeep Content-based Models\nSpotify Example\n\n^cnn with audio spectrogram as input data.\nscience.stats.Deep Neural Networks.Max Pooling and global pooling\n‘Bottleneck’ layer science.stats.Deep Neural Networks.Auto-encoders\nscience.math.Optimization.Optimizers In in Neural Networks.ReLU\nOutput is the factor vector for the track from a trained Collaborative Filtering model. It’s a model stack…science.stats.Stacking Models\n\nDeep Factorization Machines\nscience.stats.Regression.Support Vector Machines\nscience.stats.Regression.Support Vector Machines.Factorization Machines\n#TODO - skip for now\nContent2Vec ^content2vec\nCombine modular sets of feature extractors into 1 item embedding. E.g.\n^alexnet,^word2vec, science.stats.Deep Neural Networks.Prod2Vec, etc.\nFrameworks/Libraries\nscience.CS.theory.Code Transformations.Differentiable and Probabalistic Programming\nscience.CS.theory.Code Transformations.Differentiable Programming.tensorflow\nscience.CS.theory.Code Transformations.Differentiable Programming.pytorch\nscience.CS.theory.Code Transformations.Differentiable Programming.jax\nscience.cs.languages.CUDA"},"science.stats.Estimators":{"title":"Estimators","links":["science.stats.Regression.Linear-Regression"],"tags":["type/sketchnote","type/note","theme/xyz"],"content":"\n\nAn estimator is a statistic (function of data) that outputs a value for some parameter of the data generating process.\nExamples:\nAnalytical Solution\n\ndv.paragraph(dv.current().visual);\n\n\n\n                  \n                  Note\n                  \n                \n\n= this.lead\n\n\nDetails\n\n\n\n\nSupporting Content\n\n\n\n\n\nBack Matter\nSource\n \n\nbased_on::\n\nReferences\n\n\n\n\nTerms\n\n\n\n\nTarget\n\n\n\n\nTasks\n \n\n\n\nQuestions\n \n\n\n\nTemplate Help\n\n\nBasic Template Structure\nHow to Use Links\nHow to Use Tags\nHow to Search Notes\nPlugins Needed\nFind Latest Updates\n"},"science.stats.Machine-Learning.Cold-Start-Problem":{"title":"Cold Start Problem","links":[],"tags":[],"content":""},"science.stats.Machine-Learning.Find-and-Answer-similar-questions":{"title":"Find and Answer similar questions","links":["science.stats.Deep-Neural-Networks.Embeddings","science.math.Functional-Analysis.High-Dimensional-Neighborhood-Search"],"tags":[],"content":"Many people use groups every day they use these groups to ask and answer questions. However, unfortunately, like the search in this group is not perfect and there might be question duplicate or might be related questions there also might be people we want to prompt who based on the previous activity and might be a good good person to try to answer this particular question how would you design such system so so that you can find an answer for similar questions in the group?\nso this is a general. This is a general kind of search question rather just leave it to this is basically a general question just basically limited on the main so I can imagine a front representation bar user type a new question and results are showing similar questions and we believe the be so we can discuss OK off-line how do we identify questions and answers of high-quality groups that can be used as knowledge source so basically how do we choose? How do we wait it down? What is the objective function then it’s an index in retrieval questions or maybe we go with the bedding and retrieve their also also identification how do I do? We identify answers so we could have a button which is called Mark answer we can use also as proxy like something that has many likes or likes or engagement by the particular ask of the question confidence estimation I would like the question answer are relevant to each other should be proposed to post any question that in which group and beyond just texture that we obviously have everything else you can think of”\nare relevant to each other should be proposed to post any question that in which group and beyond just texture that we obviously have everything else you can think of.\nscience.stats.Deep Neural Networks.Embeddings\nscience.math.Functional Analysis.High Dimensional Neighborhood Search"},"science.stats.Machine-Learning.Industry-Applications.ChatBots.Large-Language-Models.LMQL":{"title":"LMQL","links":[],"tags":[],"content":"syntaxt\n” where …\nClauses\nwhere clause\nspecify some constraints on the output\nthe main program caluse\nBasically we can have some holes inn a text and some python expressions inside, where we’re predicting the masked words.\nThen as the model is generating stuff according to the strategy, there is a mechanism to enforce the constraints.\nHow does this work? with partial evaluation.\nFirst note how LLM’s generate their output.\nBefore I thought they generate it trough only a deterministic greedy procedure, but actually they can use\nother graph search algorithms, where the log-probablity of the output is the weight function in the inference graph. So they can do beam search, greedy search, etc.\nAs they go they know what variable is being ‘generated’ at the moment. This variable has some current value and on the basis of this one can say if the variable is violating some constraints. If yes, the search procedure can backtrack. If not, it can continue, while also knowing if the variable can have something added to it, or not.\nI think these are generated as ‘follow’ and ‘fin’ nodes in the graph for a particular variable.\nExamples:\nIf a var is constrained to be in a list (e.g. ‘positive’ or ‘negtive’) that can work in an obvious way..\nother things…\nIt saves 26-60% ofthe cost, but sort of a ‘normal amound’, as it can cut off unsatisfiable branches.\nquite flexeble with the constraints, have to learn more about the syntax…\nThere was this syntax\nsample(temperature=1.2)\n&quot;A few things not to forget when going to the sea (not travelling):\\n &quot;\n&quot;-[THING]&quot; where stops_at(thing,&quot;\\n&quot;)\n&quot;-[THING]&quot; where stops_at(thing,&quot;\\n&quot;)\n&quot;-[THING]&quot; where stops_at(thing,&quot;\\n&quot;)\n&quot;-[THING]&quot; where stops_at(thing,&quot;\\n&quot;)\n\nwhere ‘thing’ is not the same everywhere.\nQuite good at enforxing consistency, eg.g by a clause like:\n&quot;A few things not to forget when going to the sea (not travelling):\\n &quot;\n\nIt’s also included in LangChain\nAs we’ve geneRated the partial outpud\ndecoder clause\nSpecity the decoding algorithm to use,\ncan be sample, argmax/or other stuff, like beam or best_k\nBeam, argmax.\n,\nsample, beam_sample, beam_var,…\nfrom clause\nspecify the model to use\nRelated work\nLanguage Model Programming\nAll sorts of chain-of-thought and similar things can be considered as part of language model programming…\nChain of thought prompting, tree of thought, and others are all part of this and instantiations of it."},"science.stats.Machine-Learning.Industry-Applications.ChatBots.Large-Language-Models.LangChain":{"title":"Notes from the documentation","links":[],"tags":[],"content":"LangChain allows to chain models into different things, giving programmability..\nBasic example- one component decides what language you’re speaking, then the next models are the ones you call..\nSome python funcitons to extract structured data from your models…\nThere can be agents that are being called into one another, performing various functions, talking to each other…\nVector db’s and embeddings…\nbuilt image-to-text using blip…\n\nActive recall was not too successful there…\nmain value propositions- compionents, and then pre-build chains\ncomponents are more interesting.\nthere is also js, and a hack thing…\n\n  graph LR\nA[ModelIO]\nB[MemoryAndPersistence]\nC[ModelPersistence]\n"},"science.stats.Machine-Learning.Industry-Applications.ChatBots.Large-Language-Models.Token-Masking":{"title":"Token Masking","links":[],"tags":[],"content":"Mask some words in a sequence and predict which tokens should replace that…\ne.g.\nfrom transformers import pipeline\nclassifier = pipeline(&quot;fill-mask&quot;)\n \nclassifier(&quot;Paris is the [MASK] of France.&quot;)"},"science.stats.Machine-Learning.Industry-Applications.ChatBots.Large-Language-Models":{"title":"chatGPT wolfram article","links":[],"tags":[],"content":"AutoGPT\nLMQL\nOther things\nHugging GPT Paper Summary\nChat with your PDFs\n"},"science.stats.Machine-Learning.Industry-Applications.ChatBots":{"title":"ChatGPT","links":["/"],"tags":[],"content":"ChatGPT is a chatbot by OpenAI, and it’s optimized for context-aware conversations.\nLLM’s and ChatGPT\nben’s bites is a nice aggregator for applications, reserach and tooling about AI, mostly around LLm’s lately…\nEvaluating CatGPT on reasoning, hallucination, and common sense\n— 9/13 times outperforms sota zero-shot models\n— good translation TO english, poor in opposite direction.\n— ok performance on low-resource languages, but not on very low resource.\n— ok at detecting misinfioformation on some covid.\n--- Bad at inductive reasoning and math.\n— multi-hop reasoning is poor, causal and analogical thinkingare good.\n— few intrinsic hallucinations, but lots of extrincis ones…\nPaper Source\n"},"science.stats.Machine-Learning.Industry-Applications":{"title":"Industry Applications","links":["educative.io","science.engineering.technologies.Natural-Language-Processing-and-IR","science.stats.Time-Series","science.engineering.technologies.graphs.Graph-Representation-Learning-Book-Notes","science.economics.Market-Impact","science.stats.Deep-Neural-Networks.Vision-Models","science.stats.Regression.Classification","science.stats.Regression.Ranking","tags/Inductive","science.engineering.technologies.graphs.Graph-Analysis","science.stats.Regression.Recommender-Systems","science.engineering.technologies.Natural-Language-Processing-and-IR.Chatbots","engineering.system_design.Search"],"tags":["Inductive"],"content":"List taken from various sources, one of which EducativeIO\n\n\nVirtual Personal Assistants science.engineering.technologies.Natural Language Processing and IR\n**\n\n\nFinance science.stats.Time Series\n** Fraud Detection science.engineering.technologies.graphs.Graph Representation Learning Book Notes\nLecture from Graph+AI conference\n** Price Prediction science.stats.Time Series\n** Trade Execution, Risk Management, and Trading science.economics.Market Impact\n\n\nSocial Media\n** Face Recognition\nHow Face Id Works… Probably\nAlso, quite unrelated with the above link:\nscience.stats.Deep Neural Networks.Vision Models\n\n\n** People you may know\n** Pages You may like\nCan pose these either as a science.stats.Regression.Classification via\nscience.stats.Regression.Ranking.\nCan also pose them as an Inductive problem in science.engineering.technologies.graphs.Graph Analysis\n\n\nRetail\n** Product recommendation via science.stats.Regression.Recommender Systems\n** Maximization of revenue by learning customer habits\n\n\nOnline Customer Support:\n** Replacement of customer support agents by science.engineering.technologies.Natural Language Processing and IR.Chatbots\n\n\nMedicine\n** Medical Diagnosis Data Skeptic Episode\n** Medical Imaging Imaging Episode\n** Drug Discovery\n** Epidemology, large-population public health\nCOVID-19 Epidemic Mitigation via Scientific Machine Learning (SciML)\n\n\n\nSearch\nengineering.system_design.Search\nData gatherig for search - keep track if you clicked on something\nor went to the 2nd page, repeated search, etc…"},"science.stats.Machine-Learning.Interpretability.Libraries":{"title":"Libraries","links":[],"tags":[],"content":"#DALEX\nXAI Github"},"science.stats.Machine-Learning.Interpretability":{"title":"Interpretability","links":[],"tags":[],"content":""},"science.stats.Machine-Learning.Kernel-Regression-Methods":{"title":"Kernel Regression Methods","links":["science.math.Functional-Analysis.Kernel-Methods"],"tags":[],"content":"Of course related to\nscience.math.Functional Analysis.Kernel Methods\n…\nThe basic idea would be to start with a adataset of the kind &quot;&quot;X”, “Y”, and then model the function we care about as :\nE(Y|X) = f(x)=E_y(y*P(Y|x))=\\int_y(f(Y|x)*ydy)=\\int_y( f_{X,Y}(y,x)*ydy/f_X(x))\nnow, approximating the integral with a sum, we get:\nf(x) = \\sum_i w_i(x) y_i\nwhere w_i(x)=f(y,x)/f(x) are the weights we need to find.\nWe can model these weights by for example modelling them as w_i(x)= k(x,x_i)/(\\sum_{i=1..}k(x,x_i)) where k is a kernel function.\nCommon further approximation is then:\nAn example would be the Gaussian kernel, where k(x,x_i) = exp(-||x-x_i||^2/2\\sigma^2). here basically the \\sigma parameter controls how much attention is paid to datapoints further away.\nSource."},"science.stats.Machine-Learning.Scientific-Machine-Learning":{"title":"Scientific Machine Learning","links":["science.math.modelling.Differential-Equations","science.cs.languages.julia","science.CS.theory.Code-Transformations.Differentiable-and-Probabalistic-Programming"],"tags":[],"content":"[[science.CS.theory.Code Transformations.Differentiable Programming\nscience.math.modelling.Differential Equations\nscience.cs.languages.julia\nscience.CS.theory.Code Transformations.Differentiable and Probabalistic Programming"},"science.stats.Machine-Learning.Vision-System-for-Car":{"title":"Vision System for Car","links":[],"tags":[],"content":"So far a vision system for driving car we may want to cover as follows first what kind of objects do we want to recognize and why what is the downstream task? How do you get training data? Might use a fleet of instrument vehicles what are the latest or computer requirements? How do we design the evaluation both online evaluation and off-line evaluation how to detect problems and co how to get labels or self supervised signal.\ncan we mine for relevant data?\nhow to detect data and domain drift?\n"},"science.stats.Machine-Learning.hirarchical-topic-classifier":{"title":"hirarchical topic classifier","links":["science.math.Norms-and-Metrics.Hyperbolic-Geometric-Space"],"tags":[],"content":"Description\nGiven a hierarchy of labels, how would you label a given item.\nQuestion Statement\nHow to obtain the ground truth?\nHow do you obtain the training data?\nHow to determine the terminal level of the label in the hierarchy?\nHow do you handle similar labels across different paths in the tree?\nLoss function?\nBonus points if candidate talks about Hyperbolic geometric space.\nHow to handle multi-modality?\nWhat is the best metric to optimize?\nher than a tree), what changes would would the above system need?\nscience.math.Norms and Metrics.Hyperbolic Geometric Space"},"science.stats.Machine-Learning":{"title":"Curve Fitting","links":["stats.Data-Science-and-ML-Process"],"tags":[],"content":"Training w/ SGD\nInput stabilization\nMl fitting\nML Process\nTransclude of stats.Data-Science-and-ML-Process"},"science.stats.Machine-Learning.private.object-recognition-in-video":{"title":"object recognition in video","links":[],"tags":[],"content":"understand task, object detection, scale, image/video can have multiple objects,\nwhat are the dowrnstream tasks: ads/recs\ntrain’ing data\n\nbounding boxes/segmentataion labels,?\n\nfor classification, shouldd be multi-label per image/candidate.\nFeatures\nVisual features.\nText features help with object labels.\nContext features.\nModeling\nThere’re man\nModeling:\nstudy some model architectures for object detection in video/images\nssd, yollo, rpn using anchor boxes.\nEvaluation\nmetrics\nprecision’/recall/roc auc/mAP.\nIOU for object detection.\n"},"science.stats":{"title":"Stats","links":[],"tags":[],"content":""}}