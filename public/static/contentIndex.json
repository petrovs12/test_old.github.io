{"about":{"title":"About This Site","links":[],"tags":[],"content":"About This Digital Garden\nWelcome to my digital garden! This is where I share my notes, thoughts, and learnings about various topics including:\n\nMathematics and Statistics\nMachine Learning and Data Science\nSystem Design and Engineering\nAnd more…\n\nWhat is a Digital Garden?\nA digital garden is a collection of notes, essays, and ideas that grow over time. Unlike traditional blogs, digital gardens:\n\nAre living documents that evolve\nDon’t have to be perfect or complete\nShow connections between different ideas\nGrow organically over time\n\nHow This Site Works\nThis site is built using:\n\nObsidian for note-taking\nQuartz for publishing\nGitHub Pages for hosting\n\nFeel free to explore the interconnected notes and thoughts!"},"index":{"title":"Welcome to My Knowledge Base","links":["about","test","science.math","science.stats"],"tags":[],"content":"Welcome to My Knowledge Base\nThis is my personal knowledge base where I share my notes and thoughts on various topics.\nRecent Notes\n\nAbout This Site\nTest Note\nMathematics\nStatistics\n\nMain Categories\n\nMathematics and Statistics\nMachine Learning\nEngineering\nSystem Design\n\nAbout\nThis is a digital garden built with Obsidian and Quartz. Feel free to explore the interconnected notes and thoughts. Learn more in the About page."},"science.math.Functional-Analysis.Kernel-Methods":{"title":"Inner Products and Hilbert Spaces","links":["science.stats.Support-Vector-Machines"],"tags":[],"content":"A Hilbert Space is a space with an inner product and one where the limits of all Cauchy sequences wrt the norm, defined by the inner product, are in the space itself.\nThe inner product defines a norm by taking the inner product of an element with itself.\nKernel Function\nLet H be a Hilbert Space.\nA function k:R^d-&gt;R^d-&gt;R is called a kernel on R^d if there exists a feature map \\phi:R^d-&gt;H, s.t. k(x,y) = &lt;\\phi(x).\\phi(y)&gt;, for all x,y\\in R^d (or a subset thereof).\nPositive Definite Kernel Matrix (Mercer)\nA function R^d-&gt;R^d-&gt;R is positve-definite iff for any a_1,a_2,...a_k \\in R, x_1,x_2,...x_k \\in R^d,k is symmetric and\n\\Sigma_{i = 1..n} \\Sigma_{j=1..n} a_i a_j k(x_i, x_j)&gt;=0\nSame as saying for any x_1,x_2,...x_k \\in R^d, the matrix with entries M_{i,j} = k(x_i,x_j) is positive definite (or semi-definite).\nfrom Mercer’s Theorem (Wikipedia)\nMercer’s Theorem\nSuppose K is a continuous symmetric non-negative definite kernel. Then there is an orthonormal basis {ei}i of L2[a, b] consisting of eigenfunctions of TK such that the corresponding sequence of eigenvalues {λi}i is nonnegative. The eigenfunctions corresponding to non-zero eigenvalues are continuous on [a, b] and K has the representation\nK(s,t)=\\sum _{j=1}^{\\infty }\\lambda _{j}\\,e_{j}(s)\\,e_{j}(t)K(s,t)\n$=\\sum {j=1}^{\\infty }\\lambda {j},e{j}(s),e{j}(t)$$\nWhere the convergence is absolute and uniform.\nSo essentially, if K is positive definite in the above sense, there exists a feature map, for which K is the inner product of the output space.\nReproducing Kernel Hilbert Spaces\nRepresenter Theorem and Regularization\nDifferent Kernels and Operations\nKernel SVM\nscience.stats.Support Vector Machines\nWe have an analogy of the original SVM dual formulation, with a generalized kernel at the place of the inner product function.\nKernel PCA\nLook at the pretty pics from sklearn\nRepresentation of Probability Densities\nPossible, but looks complicated from the lecture notes :D"},"science.math":{"title":"Mathematics","links":["science.math.Functional-Analysis.Kernel-Methods","science.math.Linear-Algebra","science.math.Optimization","science.math.calculus","root.science.CS"],"tags":[],"content":"Mathematics\nWelcome to the mathematics section of the knowledge base. Here you’ll find various topics related to mathematics, including:\nTopics\n\nKernel Methods and Functional Analysis\nLinear Algebra\nOptimization\nCalculus\n\nOverview\nThis section contains notes and explanations about various mathematical concepts, particularly those relevant to machine learning, data science, and computer science.\nFeel free to explore the different topics through the links above.\nroot.science.CS"}}