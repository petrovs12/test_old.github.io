---
id: cvii3wwvwshj9id6bbtpbyv
title: RLHF
desc: ''
updated: 1734698645021
created: 1734693477818
---

So regarding the Reinforcement learning with human feedback situation. Here is how it maps to the traditional. Reinforcement learning framework. So. What we have is. Basically state space. Action space. Policy transition function. And reward is coming from the human feedback raters-
rating responses 1 to 5.


is it very different from fine tuning? isnt the policy gradient very different to stochastic gradient descent? 



