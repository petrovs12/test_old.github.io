---
id: o4s5e9s5gqlovmv18cer7ml
title: Semi-supervised Learning
desc: ""
updated: 2024-12-15T22:35
created: 1642424935354
---


OK, so as far as I understand for semi supervised learning that is as follows, we have a small number of labeled examples where the objective function is as usual not predict correctly so we have an binary low likelihood, etc. etc. now for the unsupervised date points that there is that small data perturba the prediction for them should not change too much so one way this could be achieve. This we produced our augmented version of the unlevel data set and perhaps of the label to everything and then we are like in the forward pass we predict the probability for each label for the label for for the label things and then in objective function, we enforsome distance between each pair of points, each predictions rather so those will be distribution so it could be like some you know matrix distance between such a sky divergent or or square root means square there also below and this is a guest applied line related to the over objective function so this is the regularizerr.

[[deep lea]]
The two parts of the optimization function are combined via some linear weights